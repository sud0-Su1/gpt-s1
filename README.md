# GPT-From-Scratch ğŸš€  
A minimal yet powerful implementation of a **Generative Pretrained Transformer (GPT)** model inspired by the seminal paper **â€œAttention is All You Needâ€** and modern transformer architectures 

---

## ğŸ¯ Project Aim

This project aims to build a compact, efficient, and interpretable transformer model from scratch using only core PyTorch components. Inspired by nanoGPT, our implementation targets ~**0.2 million parameters**, making it lightweight enough to train on a single GPU or CPU.

The model is trained on the **Tiny Shakespeare** dataset and optimized to generate coherent, relevant, and stylistically consistent text.

---

## ğŸ“š Dataset

- **Name**: [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)  
- **Size**: ~1MB (character-level text corpus)  
- **Format**: Plain text (.txt)

---

## ğŸ§  Model Overview

- Architecture: Transformer Decoder (GPT-style)
- Components:
  - Multi-Head Self-Attention
  - Positional Encoding
  - Layer Normalization
  - Feedforward Layers
- Parameters: ~0.2M
- Training Objective: Next-token prediction using **causal language modeling**
- Optimizer: AdamW
- Loss Function: Cross Entropy Loss

---

## ğŸ—ï¸ Architecture Inspiration

- **"Attention is All You Need"** (Vaswani et al.)
- **GPT-2/GPT-3** by OpenAI
- **nanoGPT** by Andrej Karpathy

Our implementation mimics the core transformer components, but is scaled down to accommodate smaller compute environments.

---

## ğŸ› ï¸ Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/GPT-From-Scratch.git
cd GPT-From-Scratch
```

2. **Create virtual environment (optional)**
```bash
python -m venv venv
source venv/bin/activate    # or venv\Scripts\activate on Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Training

```bash
python train.py --epochs 20 --batch_size 64 --lr 3e-4 --block_size 128
```

- `train.py` handles tokenization, model training, checkpoint saving
- Uses PyTorch and runs on CPU or GPU (if available)

---

## ğŸ“ˆ Evaluation & Generation

After training completes:

```bash
python generate.py --checkpoint checkpoints/model.pt --start "ROMEO:" --length 200
```

Output:
```
ROMEO:
What light through yonder window breaks? It is the east...
```

---

## ğŸ—‚ï¸ Directory Structure

```
GPT-From-Scratch/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tinyshakespeare.txt
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gpt.py           # Transformer model
â”‚   â””â”€â”€ config.py        # Model config
â”œâ”€â”€ train.py             # Training script
â”œâ”€â”€ generate.py          # Text generation script
â”œâ”€â”€ utils.py             # Helper functions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

Edit `config.py` to adjust model parameters:

```python
vocab_size = 65          # Number of unique tokens
n_embed = 128            # Embedding dimension
n_heads = 4              # Attention heads
n_layers = 4             # Transformer layers
block_size = 128         # Context length
dropout = 0.1
```

---

## ğŸ“Œ Future Work

- Add training visualization with TensorBoard
- Experiment with larger datasets
- Implement mixed-precision training
- Add tokenizer support for subword units

---

